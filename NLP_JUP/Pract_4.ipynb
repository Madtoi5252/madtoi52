{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ed9121-3990-4b89-bb93-e66ba11b0137",
   "metadata": {},
   "source": [
    "**4 a): Tokenization using Python’s split() function**`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81cf376b-c4d8-4034-8533-a2364321d34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Founded', 'in', '2002,', 'SpaceX’s', 'mission', 'is', 'to', 'enable', 'humans', 'to', 'become', 'a', 'spacefaring', 'civilization', 'and', 'a', 'multi-planet', 'species', 'by', 'building', 'a', 'self-sustaining', 'city', 'on', 'Mars.', 'In', '2008,', 'SpaceX’s', 'Falcon', '1', 'became', 'the', 'first', 'privately', 'developed', 'liquid-fuel', 'launch', 'vehicle', 'to', 'orbit', 'the', 'Earth.']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
    "# Splits at space\n",
    "a=text.split()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5742f4-97a7-439f-a275-ee78776a8958",
   "metadata": {},
   "source": [
    "**4 b): Tokenization using Regular Expressions (RegEx)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b764a77-8fa5-4d98-9322-8a986d90cfd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'is',\n",
       " 'fascinating',\n",
       " 'Do',\n",
       " 'you',\n",
       " 'agree']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"Natural language processing is fascinating, Do you agree ?\"\"\"\n",
    "tokens = re.findall(\"[\\w]+\", text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf5520-d840-454b-a456-d0e443cc1a8f",
   "metadata": {},
   "source": [
    "**4 c): Tokenization using NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa3f1b92-21e7-4848-9c76-5b55c7e34727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Practical using NLTK Word Tokenize.\n",
      "['Machine', 'learning', 'enables', 'computers', 'to', 'learn', 'from', 'data', '.', 'It', 'is', 'a', 'key', 'component', 'of', 'artificial', 'intelligence', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prath\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"\"\"Machine learning enables computers to learn from data. It is a key component of artificial intelligence.\"\"\"\n",
    "a=word_tokenize(text)\n",
    "print(\"This is Practical using NLTK Word Tokenize.\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8043fd98-aa48-4ba1-8fec-e6001c161793",
   "metadata": {},
   "source": [
    "**4 d) Tokenization using the spaCy library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7588c97b-342f-4eff-ac92-eec055657ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine', 'learning', 'enables', 'computers', 'to', 'learn', 'from', 'data', '.', 'It', 'is', 'a', 'key', 'component', 'of', 'artificial', 'intelligence', '.']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "text = \"\"\"Machine learning enables computers to learn from data. It is a key component of artificial intelligence.\"\"\"\n",
    "\n",
    "my_doc = nlp(text)\n",
    "token_list = []\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "\n",
    "token_list\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72d4e88-d1a8-4bb1-8480-665cf4fc127f",
   "metadata": {},
   "source": [
    "**4 e) Tokenization using Keras.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27a3b264-eb63-48bd-896f-ab00d0e65503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine', 'learning', 'enables', 'computers', 'to', 'learn', 'from', 'data', 'it', 'is', 'a', 'key', 'component', 'of', 'artificial', 'intelligence']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "text = \"\"\"Machine learning enables computers to learn from data. It is a key component of artificial intelligence.\"\"\"\n",
    "result = text_to_word_sequence(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d5308-39e0-428d-9a98-23ccd7da5c00",
   "metadata": {},
   "source": [
    "**4 f)Tokenization using Gensim**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff7230ef-3b11-46ed-b939-f662b9f38787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine',\n",
       " 'learning',\n",
       " 'enables',\n",
       " 'computers',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'from',\n",
       " 'data',\n",
       " 'It',\n",
       " 'is',\n",
       " 'a',\n",
       " 'key',\n",
       " 'component',\n",
       " 'of',\n",
       " 'artificial',\n",
       " 'intelligence']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "text = \"\"\"Machine learning enables computers to learn from data. It is a key component of artificial intelligence.\"\"\"\n",
    "list(tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d53ca-38dc-42dd-9a66-df92c623eb63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
