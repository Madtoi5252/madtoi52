{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "402f8f79-5bab-40e0-841b-8712984f1fa9",
   "metadata": {},
   "source": [
    "**PART A : STUDY PORTER STEMMER, LANCASTER STEMMER, REGEXP STEMMER \r\n",
    "AND SNOWBALL STEMMER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366f1802-44ac-4662-b000-7055692d0414",
   "metadata": {},
   "source": [
    "**a. Porter Stemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "804c8a73-0999-409e-9be3-3bfe5806bcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene\n",
      "gene\n",
      "genesi\n",
      "genet\n",
      "gener\n",
      "gener\n",
      "\n",
      " Performing porter stemming on a sentence\n",
      "heya\n",
      "raj\n",
      ",\n",
      "today\n",
      "is\n",
      "our\n",
      "practic\n",
      "exam\n",
      ",\n",
      "we\n",
      "are\n",
      "veri\n",
      "much\n",
      "excit\n",
      ".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prath\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "terms = [\"gene\", \"genes\", \"genesis\", \"genetic\", \"generic\", \"general\"]\n",
    "\n",
    "for each_term in terms:\n",
    "    print(porter.stem(each_term))\n",
    "\n",
    "sentence = \"Heya Raj, today is our practical exams, we are very much excited.\"\n",
    "\n",
    "print(\"\\n Performing porter stemming on a sentence\")\n",
    "words = word_tokenize(sentence, language = 'english')\n",
    "for each_word in words:\n",
    "    print(porter.stem(each_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce565951-407c-47c2-bfa5-6791aae43b87",
   "metadata": {},
   "source": [
    "**b. Lancaster Stemmer**`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa5d4ed-ca43-40a4-a1e7-8d20890ae508",
   "metadata": {},
   "source": [
    "Token.txt (This text file is used for observing the results of stemming on a file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b5a8555-2653-4fa2-817c-0c170ecf36b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Performing lancaster stemming on the words\n",
      "enjoy\n",
      "enjoy\n",
      "enjoy\n",
      "enjoy\n",
      "enjoy\n",
      "enjoy\n",
      "\n",
      ". Performing lancaster stemming on a sentence\n",
      "hey\n",
      "raj\n",
      ",\n",
      "today\n",
      "is\n",
      "our\n",
      "pract\n",
      "exam\n",
      ",\n",
      "we\n",
      "ar\n",
      "very\n",
      "much\n",
      "excit\n",
      ".\n",
      "\n",
      " Performing lancaster stemming on a text file - one sentence at a time\n",
      "al\n",
      "was\n",
      "begin\n",
      "to\n",
      "get\n",
      "very\n",
      "tir\n",
      "of\n",
      "sit\n",
      "by\n",
      "her\n",
      "sist\n",
      "on\n",
      "the\n",
      "bank\n",
      "and\n",
      "of\n",
      "hav\n",
      "noth\n",
      "to\n",
      "do\n",
      "ont\n",
      "or\n",
      "twic\n",
      "she\n",
      "had\n",
      "peep\n",
      "into\n",
      "the\n",
      "book\n",
      "her\n",
      "sist\n",
      "was\n",
      "read\n",
      "but\n",
      "it\n",
      "had\n",
      "no\n",
      "pict\n",
      "or\n",
      "convers\n",
      "in\n",
      "it\n",
      ".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prath\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "terms = [\"enjoy\", \"enjoying\", \"enjoyed\", \"enjoyable\", \"enjoyment\", \"enjoyful\"]\n",
    "\n",
    "print(\"\\n Performing lancaster stemming on the words\")\n",
    "for each_term in terms:\n",
    "    print(lancaster.stem(each_term))\n",
    "\n",
    "sentence = \"Heya Raj, today is our practical exams, we are very much excited.\"\n",
    "\n",
    "print(\"\\n. Performing lancaster stemming on a sentence\")\n",
    "words = word_tokenize(sentence, language = 'english')\n",
    "for each_word in words:\n",
    "    print(lancaster.stem(each_word))\n",
    "\n",
    "print(\"\\n Performing lancaster stemming on a text file - one sentence at a time\")\n",
    "# Treating the text file as a collection of sentences\n",
    "file = open(\"Token.txt\")\n",
    "my_lines_list = file.readlines()\n",
    "# Accessing one line at a time from the text file\n",
    "words = word_tokenize(my_lines_list[0], language = 'english')\n",
    "for each_word in words:\n",
    "    print(lancaster.stem(each_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c590c83f-19c1-45bc-9538-e5e06cdf6915",
   "metadata": {},
   "source": [
    "**c. Snowball Stemmer**n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1f3ac4c-233e-433a-b581-501dc6e6d099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Performing snowball stemming one word\n",
      "hybernat\n",
      "\n",
      " Performing snowball stemming on english language words\n",
      "raju\n",
      "cheerful\n",
      "bravery\n",
      "drawing\n",
      "satisfactorily\n",
      "publisher\n",
      "painful\n",
      "hardwork\n",
      "key\n",
      "\n",
      " Performing snowball stemming on dutch language words\n",
      "raju\n",
      "bess\n",
      "vriendelijk\n",
      "hobbel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prath\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "snowball_english = SnowballStemmer(\"english\")\n",
    "snowball_english = SnowballStemmer(\"dutch\")\n",
    "\n",
    "print(\"\\n Performing snowball stemming one word\")\n",
    "word = snowball_english.stem(\"Hybernating\")\n",
    "print(word)\n",
    "\n",
    "terms = [\"Raju\", \"cheerful\", \"bravery\",\"drawing\", \"satisfactorily\", \"publisher\", \"painful\", \"hardworking\",\n",
    "\"keys\"]\n",
    "\n",
    "print(\"\\n Performing snowball stemming on english language words\")\n",
    "for each_term in terms:\n",
    "    print(snowball_english.stem(each_term))\n",
    "\n",
    "terms2 = [\"Raju\", \"bessen\", \"vriendelijkheid\", \"hobbelig\"]\n",
    "\n",
    "print(\"\\n Performing snowball stemming on dutch language words\")\n",
    "for each_term in terms2:\n",
    "    print(snowball_english.stem(each_term))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedcdc86-7b6d-4d48-acef-3aa734668c8e",
   "metadata": {},
   "source": [
    "**d. RegExp Stemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5eca007-c0b9-4fea-8725-35a1a0067ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Performing regexp stemming on one word at a time\n",
      "car\n",
      "bee\n",
      "comput\n",
      "\n",
      "2. Performing regexp stemming on a list of words\n",
      "Raj\n",
      "stemm\n",
      "physical\n",
      "easy\n",
      "popstar\n",
      "effort\n",
      "aknowledge\n",
      "vegetable\n",
      "advis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prath\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "regexp = RegexpStemmer('ing$|s$|e$|able$|ment$|less$|ly$', min=4)\n",
    "\n",
    "print(\"\\n Performing regexp stemming on one word at a time\")\n",
    "print(regexp.stem('cars'))\n",
    "print(regexp.stem('bees'))\n",
    "print(regexp.stem('compute'))\n",
    "\n",
    "terms = [\"Rajs\", \"stemming\", \"physically\", \"easy\",\"popstar\", \"effortless\", \"aknowledgement\",\"vegetables\",\n",
    "\"advisable\"]\n",
    "\n",
    "print(\"\\n2. Performing regexp stemming on a list of words\")\n",
    "for each_term in terms:\n",
    "    print(regexp.stem(each_term))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9535aed-7944-4b3a-ac63-81f611fb4cdd",
   "metadata": {},
   "source": [
    "**PART B : STUDY WORDNET LEMMATIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "328358ec-537e-41af-8bf4-62feae8584e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Performing WordNet lemmatization on single Words\n",
      "corpus\n",
      "best\n",
      "goose\n",
      "foot\n",
      "cactus\n",
      "\n",
      " Performing WordNet lemmatization on a sentence\n",
      "\n",
      "Converting the sentence into a list of words\n",
      "['Hey', 'Raj', ',', 'how', 'are', 'you', 'doing', '?', 'Keep', 'learning', 'continously', 'for', 'better', 'result', '!']\n",
      "\n",
      "After applying wordnet lemmatizer, the result is....\n",
      "Hey Raj , how be you do ? Keep learn continously for better result !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\prath\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prath\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "print(\"\\n1. Performing WordNet lemmatization on single Words\")\n",
    "print(wordnet.lemmatize(\"corpora\"))\n",
    "print(wordnet.lemmatize(\"best\"))\n",
    "print(wordnet.lemmatize(\"geese\"))\n",
    "print(wordnet.lemmatize(\"feet\"))\n",
    "print(wordnet.lemmatize(\"cacti\"))\n",
    "\n",
    "print(\"\\n Performing WordNet lemmatization on a sentence\")\n",
    "\n",
    "sentence = \"Hey Raj, how are you doing? Keep learning continously for better result!\"\n",
    "\n",
    "list_words = nltk.word_tokenize(sentence)\n",
    "print(\"\\nConverting the sentence into a list of words\")\n",
    "print(list_words)\n",
    "\n",
    "final = ' '.join([wordnet.lemmatize(each_word, pos = 'v') for each_word in list_words])\n",
    "print(\"\\nAfter applying wordnet lemmatizer, the result is....\")\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274ab01-1630-466c-8f75-7d4f183e6c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
