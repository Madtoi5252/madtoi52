{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0e380e-d9b4-4c62-8280-cfac0372073c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, True, True, True, True]\n",
      "1/1 [==============================] - 15s 15s/step\n",
      "[0.01760279]\n",
      "1/1 [==============================] - 0s 442ms/step\n",
      "[0.01760279]\n",
      "Epoch 1/10\n",
      "391/391 [==============================] - 3326s 8s/step - loss: 0.6595 - accuracy: 0.5520 - val_loss: 0.5286 - val_accuracy: 0.7260\n",
      "Epoch 2/10\n",
      "217/391 [===============>..............] - ETA: 11:38 - loss: 0.4742 - accuracy: 0.7622"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history['val_' + metric], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, 'val_' + metric])\n",
    "\n",
    "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "VOCAB_SIZE = 1000\n",
    "encoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()), output_dim=64, mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "print([layer.supports_masking for layer in model.layers])\n",
    "\n",
    "# Predict on a sample text without padding\n",
    "sample_text = ('The movie was cool. The animation and the graphics were out of this world. I would recommend this movie.')\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])\n",
    "\n",
    "# Predict on a sample text with padding\n",
    "padding = \"the \" * 2000\n",
    "predictions = model.predict(np.array([sample_text, padding]))\n",
    "print(predictions[0])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=test_dataset, validation_steps=30)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)\n",
    "\n",
    "# Plot training and validation metrics\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.ylim(0, None)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "# Compile the model with stacked LSTM layers\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model with stacked LSTM layers\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=test_dataset, validation_steps=30)\n",
    "\n",
    "# Evaluate the model with stacked LSTM layers\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)\n",
    "\n",
    "# Plot training and validation metrics for the stacked model\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb687e3a-f50a-4882-93f2-3a19d4699b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
